{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d501a1-4df7-4048-8169-437655a74322",
   "metadata": {},
   "source": [
    "# Climate Wavers Vision Model: WaverX-Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfc9e8c-8f3d-41cb-b4e4-0868d576e8fe",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20f5c2fc-a07e-480c-93fe-73bd45bc76d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\n",
      "   KMP_BLOCKTIME=1\n",
      "   KMP_SETTINGS=1\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=128\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=1\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_ENABLE_TASK_THROTTLING=true\n",
      "   KMP_FORCE_MONOTONIC_DYNAMIC_SCHEDULE=false\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_FORKJOIN_FRAMES=true\n",
      "   KMP_FORKJOIN_FRAMES_MODE=3\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_ITT_PREPARE_DELAY=0\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_MWAIT_HINTS=0\n",
      "   KMP_NESTING_MODE=0\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=8M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_PROC_BIND=spread\n",
      "   KMP_TEAMS_THREAD_LIMIT=8\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_TPAUSE=0\n",
      "   KMP_USER_LEVEL_MWAIT=false\n",
      "   KMP_USE_YIELD=1\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=true\n",
      "   LIBOMP_NUM_HIDDEN_HELPER_THREADS=8\n",
      "   LIBOMP_USE_HIDDEN_HELPER_TASK=true\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %i thread %n bound to OS proc set {%A}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=1\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED: deprecated; max-active-levels-var=1\n",
      "   OMP_NUM_TEAMS=0\n",
      "   OMP_NUM_THREADS: value is not defined\n",
      "   OMP_PLACES='threads'\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=8M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_TEAMS_THREAD_LIMIT=0\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_TOOL=enabled\n",
      "   OMP_TOOL_LIBRARIES: value is not defined\n",
      "   OMP_TOOL_VERBOSE_INIT: value is not defined\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='verbose,warnings,respect,granularity=thread,compact,1,0'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: numpy in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.26.1)\n",
      "Collecting imutils\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: opencv-python-headless==4.8.1.78 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (4.8.1.78)\n",
      "Requirement already satisfied: ovmsclient==2023.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (2023.1)\n",
      "Requirement already satisfied: matplotlib in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (3.5.2)\n",
      "Requirement already satisfied: scikit-learn==1.3.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: openvino==2023.1.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (2023.1.0)\n",
      "Requirement already satisfied: tensorflow in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (2.14.0)\n",
      "Requirement already satisfied: ipython in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (8.5.0)\n",
      "Requirement already satisfied: addict in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (2.4.0)\n",
      "Requirement already satisfied: protobuf>=3.19.4 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ovmsclient==2023.1->-r requirements.txt (line 5)) (4.25.0)\n",
      "Requirement already satisfied: requests>=2.27.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ovmsclient==2023.1->-r requirements.txt (line 5)) (2.31.0)\n",
      "Requirement already satisfied: grpcio>=1.47.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ovmsclient==2023.1->-r requirements.txt (line 5)) (1.59.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from scikit-learn==1.3.1->-r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from scikit-learn==1.3.1->-r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from scikit-learn==1.3.1->-r requirements.txt (line 7)) (1.11.3)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.1.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from openvino==2023.1.0->-r requirements.txt (line 8)) (2023.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2022.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (2.0.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (16.0.6)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (58.0.4)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (3.6.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (1.13.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (0.5.4)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (2.14.1)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (20210226132247)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (4.1.1)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (2.14.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (0.34.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (2.14.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (3.3.0)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (4.8.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (2.13.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (3.0.31)\n",
      "Requirement already satisfied: pickleshare in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (0.7.5)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (0.1.6)\n",
      "Requirement already satisfied: stack-data in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (0.5.0)\n",
      "Requirement already satisfied: traitlets>=5 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (5.4.0)\n",
      "Requirement already satisfied: decorator in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (0.18.1)\n",
      "Requirement already satisfied: backcall in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 9)) (0.37.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from jedi>=0.16->ipython->-r requirements.txt (line 10)) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from pexpect>4.3->ipython->-r requirements.txt (line 10)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython->-r requirements.txt (line 10)) (0.2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from requests>=2.27.1->ovmsclient==2023.1->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from requests>=2.27.1->ovmsclient==2023.1->-r requirements.txt (line 5)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from requests>=2.27.1->ovmsclient==2023.1->-r requirements.txt (line 5)) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from requests>=2.27.1->ovmsclient==2023.1->-r requirements.txt (line 5)) (1.26.11)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (0.7.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (3.3.4)\n",
      "Requirement already satisfied: asttokens in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from stack-data->ipython->-r requirements.txt (line 10)) (2.0.8)\n",
      "Requirement already satisfied: pure-eval in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from stack-data->ipython->-r requirements.txt (line 10)) (0.2.2)\n",
      "Requirement already satisfied: executing in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from stack-data->ipython->-r requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (3.2.0)\n",
      "Building wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25858 sha256=99605059c22730715a60267afb540e774e508cbd6b8163e38e358ca2bde1af7d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cq3lr_nw/wheels/4b/a5/2d/4a070a801d3a3d93f033d3ee9728f470f514826e89952df3ea\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a44f8d-fd36-4b5f-bdde-ff4b2a56ca3e",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529dad9-2c46-4d58-9629-3af1d4bb55b8",
   "metadata": {},
   "source": [
    "### Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22a31375-8fdc-4c37-927a-1201b8c319ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import os\n",
    "# initialize the path to the *original* input directory of images\n",
    "INPUT_DATASET = \"dataset\"\n",
    "# initialize the base path to the *new* directory that will contain\n",
    "# our images after computing the training and testing split\n",
    "BASE_PATH = \"model_dataset\"\n",
    "# derive the training, validation, and testing directories\n",
    "TRAIN_PATH = os.path.sep.join([BASE_PATH, \"training\"])\n",
    "VAL_PATH = os.path.sep.join([BASE_PATH, \"validation\"])\n",
    "TEST_PATH = os.path.sep.join([BASE_PATH, \"testing\"])\n",
    "\n",
    "# define the amount of data that will be used training\n",
    "TRAIN_SPLIT = 0.75\n",
    "# the amount of validation data will be a percentage of the\n",
    "# *training* data\n",
    "VAL_SPLIT = 0.1\n",
    "# define the names of the classes\n",
    "CLASSES = [\"Earthquake\", \"Drought\",\n",
    "           \"Damaged Infrastructure\", \"Human Damage\", \"Human\", \"Land Slide\", \"Non Damage Buildings and  Street\", \"Non Damage Wildlife Forest\",\n",
    "           \"Sea\", \"Urban Fire\", \"Wild Fire\", \"Water Disaster\"]\n",
    "\n",
    "CLASSES.sort()\n",
    "\n",
    "# initialize the initial learning rate, batch size, and number of\n",
    "# epochs to train for\n",
    "INIT_LR = 1e-4\n",
    "BS = 32\n",
    "NUM_EPOCHS = 50\n",
    "UNFROZEN_NUM_EPOCHS = 20\n",
    "# define the path to the serialized output model after training\n",
    "MODEL_PATH = \"model\"\n",
    "# define the path to the output training history plots\n",
    "UNFROZEN_PLOT_PATH = os.path.sep.join([\"output\", \"plots/unfrozen.png\"])\n",
    "WARMUP_PLOT_PATH = os.path.sep.join([\"output\", \"plots/warmup.png\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a269fb8-58a9-4c8e-8b73-d2bd7af805c7",
   "metadata": {},
   "source": [
    "### Dataset Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afc038d7-c88a-4431-8d9d-37dc215873b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building 'training' split\n",
      "[INFO] building 'validation' split\n",
      "[INFO] building 'testing' split\n",
      "[INFO] Total training data === 12428\n",
      "[INFO] Total valid training data === 12428\n",
      "[INFO] Total validating data === 2209\n",
      "[INFO] Total valid validating data === 2209\n",
      "[INFO] Total testing data === 5913\n",
      "[INFO] Total valid testing data === 5913\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import config\n",
    "from imutils import paths\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#grab the paths to all input images in the original input directory\n",
    "# and shuffle them\n",
    "imagePaths = list(paths.list_images(config.INPUT_DATASET))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "# compute the training and testing split\n",
    "index = int(len(imagePaths) * config.TRAIN_SPLIT)\n",
    "trainPaths = imagePaths[:index]\n",
    "testPaths = imagePaths[index:]\n",
    "# we'll be using part of the training data for validation\n",
    "index = int(len(trainPaths) * config.VAL_SPLIT)\n",
    "valPaths = trainPaths[:index]\n",
    "trainPaths = trainPaths[index:]\n",
    "# define the datasets that we'll be building\n",
    "datasets = [\n",
    "    (\"training\", trainPaths, config.TRAIN_PATH),\n",
    "    (\"validation\", valPaths, config.VAL_PATH),\n",
    "    (\"testing\", testPaths, config.TEST_PATH)\n",
    "]\n",
    "\n",
    "# loop over the datasets\n",
    "for (dType, imagePaths, baseOutput) in datasets:\n",
    "    # show which data split we are creating\n",
    "    print(\"[INFO] building '{}' split\".format(dType))\n",
    "    # if the output base output directory does not exist, create it\n",
    "    if not os.path.exists(baseOutput):\n",
    "        print(\"[INFO] 'creating {}' directory\".format(baseOutput))\n",
    "        os.makedirs(baseOutput)\n",
    "    # loop over the input image paths\n",
    "    for inputPath in imagePaths:\n",
    "        # extract the filename of the input image along with its\n",
    "        # corresponding class label\n",
    "        filename = inputPath.split(os.path.sep)[-1]\n",
    "        label = inputPath.split(os.path.sep)[-2]\n",
    "        # build the path to the label directory\n",
    "        labelPath = os.path.sep.join([baseOutput, label])\n",
    "        # if the label output directory does not exist, create it\n",
    "        if not os.path.exists(labelPath):\n",
    "            print(\"[INFO] 'creating {}' directory\".format(labelPath))\n",
    "            os.makedirs(labelPath)\n",
    "        # construct the path to the destination image and then copy\n",
    "        # the image itself\n",
    "        p = os.path.sep.join([labelPath, filename])\n",
    "        shutil.copy2(inputPath, p)\n",
    "\n",
    "# Total number of image paths in training, validation,\n",
    "# and testing directories\n",
    "totalTrain = len(list(paths.list_images(config.TRAIN_PATH)))\n",
    "totalVal = len(list(paths.list_images(config.VAL_PATH)))\n",
    "totalTest = len(list(paths.list_images(config.TEST_PATH)))\n",
    "\n",
    "\n",
    "def filter_valid_images(directory):\n",
    "    valid_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        for file in os.listdir(file_path):\n",
    "            try:\n",
    "                # Attempt to open the file as an image\n",
    "                Image.open(f'{file_path}/{file}')\n",
    "                current_time = datetime.now()\n",
    "                formatted_time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                valid_files.append(file)\n",
    "                # Log results\n",
    "                with open('dataset_filtering.log', 'a') as log:\n",
    "                    log.write(f'{file} is valid -[{formatted_time}]\\n')\n",
    "            except OSError as e:\n",
    "                # Handle exceptions and log the error\n",
    "                image_path = os.path.join(file_path, file)\n",
    "                current_time = datetime.now()\n",
    "                formatted_time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                os.remove(image_path)\n",
    "                with open('dataset_filtering.log', 'w') as log:\n",
    "                    log.write(f\"Skipping file {file}: {e} -[{formatted_time}]\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle other exceptions\n",
    "                image_path = os.path.join(file_path, file)\n",
    "                current_time = datetime.now()\n",
    "                formatted_time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                os.remove(image_path)\n",
    "                with open('dataset_filtering.log', 'w') as log:\n",
    "                    log.write(f\"Unexpected error: {e} -[{formatted_time}]\\n\")\n",
    "    return len(valid_files)\n",
    "\n",
    "valid_train = filter_valid_images(config.TRAIN_PATH)\n",
    "print(\"[INFO] Total training data === {}\".format(totalTrain))\n",
    "print(\"[INFO] Total valid training data === {}\".format(valid_train))\n",
    "valid_val = filter_valid_images(config.VAL_PATH)\n",
    "print(\"[INFO] Total validating data === {}\".format(totalVal))\n",
    "print(\"[INFO] Total valid validating data === {}\".format(valid_val))\n",
    "valid_test = filter_valid_images(config.TEST_PATH)\n",
    "print(\"[INFO] Total testing data === {}\".format(totalTest))\n",
    "print(\"[INFO] Total valid testing data === {}\".format(valid_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d13fc-87cf-4fe7-9d0a-d1dd4f70a6ba",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cbbb5e7-e463-4342-ad73-00c406f6fcaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m paths\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResNet50\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imutils'"
     ]
    }
   ],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report\n",
    "import config\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "LAYERS_TO_FREEZE = 172\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
    "                help=\"path to output loss/accuracy plot\")\n",
    "args = vars(ap.parse_args(args=[]))\n",
    "# Total number of image paths in training, validation,\n",
    "# and testing directories\n",
    "total_train = len(list(paths.list_images(config.TRAIN_PATH)))\n",
    "total_val = len(list(paths.list_images(config.VAL_PATH)))\n",
    "total_test = len(list(paths.list_images(config.TEST_PATH)))\n",
    "\n",
    "\n",
    "def freeze_layer(model, base_model):\n",
    "    \"\"\"Freeze all layers and compile the model\"\"\"\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    opt = Adam(learning_rate=config.INIT_LR, decay=config.INIT_LR / config.NUM_EPOCHS)\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def new_layer(base_model):\n",
    "    # construct the new layer of the model that will be placed on top of the\n",
    "    # the base model\n",
    "    top_model = base_model.output\n",
    "    top_model = AveragePooling2D(pool_size=(7, 7))(top_model)\n",
    "    top_model = Flatten(name=\"flatten\")(top_model)\n",
    "    top_model = Dense(256, activation=\"relu\")(top_model)\n",
    "    top_model = Dropout(0.5)(top_model)\n",
    "    top_model = Dense(len(config.CLASSES), activation=\"softmax\")(top_model)\n",
    "    model = Model(inputs=base_model.input, outputs=top_model)\n",
    "    return model\n",
    "\n",
    "def unfreeze_layer(baseModel, model):\n",
    "    for layer in baseModel.layers[15:]:\n",
    "        layer.trainable = True\n",
    "    # loop over the layers in the model and show which ones are trainable\n",
    "    # or not\n",
    "    for layer in baseModel.layers:\n",
    "        print(\"{}: {}\".format(layer, layer.trainable))\n",
    "    # for the changes to the model to take affect we need to recompile\n",
    "    # the model, this time using SGD with a *very* small learning rate\n",
    "    print(\"[INFO] re-compiling model...\")\n",
    "    opt = ADAM(learning_rate=config.INIT_LR, decay=config.INIT_LR / config.UNFROZEN_NUM_EPOCHS)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def train_model(model, epochs):\n",
    "    # train the model\n",
    "    print(\"[INFO] training model...\")\n",
    "    H = model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=total_train // config.BS,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=total_val // config.BS,\n",
    "        epochs=epochs)\n",
    "\n",
    "    return H\n",
    "\n",
    "def plot_training(H, N, plot_path):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(plot_path)\n",
    "\n",
    "print(\"Total training data === {}\".format(total_train))\n",
    "print(\"Total validating data === {}\".format(total_val))\n",
    "print(\"Total testing data === {}\".format(total_test))\n",
    "\n",
    "# initialize the training training data augmentation object\n",
    "train_aug = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")\n",
    "# initialize the validation/testing data augmentation object (which\n",
    "# we'll be adding mean subtraction to)\n",
    "val_aug = ImageDataGenerator()\n",
    "# define the ImageNet mean subtraction (in RGB order) and set the\n",
    "# the mean subtraction value for each of the data augmentation\n",
    "# objects\n",
    "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
    "train_aug.mean = mean\n",
    "val_aug.mean = mean\n",
    "\n",
    "\n",
    "# initialize the training generator\n",
    "train_gen = train_aug.flow_from_directory(\n",
    "    config.TRAIN_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    batch_size=config.BS)\n",
    "# initialize the validation generator\n",
    "val_gen = val_aug.flow_from_directory(\n",
    "    config.VAL_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=config.BS)\n",
    "# initialize the testing generator\n",
    "test_gen = val_aug.flow_from_directory(\n",
    "    config.TEST_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=config.BS)\n",
    "\n",
    "\n",
    "# off\n",
    "print(\"[INFO] preparing model...\")\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False,\n",
    "                     input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# place the top FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = new_layer(base_model)\n",
    "model = freeze_layer(model, base_model)\n",
    "\n",
    "\n",
    "H = train_model(model, config.NUM_EPOCHS)\n",
    "# reset the testing generator and then use our trained model to\n",
    "# make predictions on the data\n",
    "print(\"[INFO] evaluating network...\")\n",
    "test_gen.reset()\n",
    "predIdxs = model.predict(test_gen, steps=(total_test // config.BS) + 1)\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(test_gen.classes, predIdxs,\n",
    "                            target_names=test_gen.class_indices.keys()))\n",
    "N = config.NUM_EPOCHS\n",
    "plot_training(H, N, config.WARMUP_PLOT_PATH)\n",
    "\n",
    "# reset our data generators\n",
    "train_gen.reset()\n",
    "val_gen.reset()\n",
    "# now that the head FC layers have been trained/initialized, lets\n",
    "# unfreeze the final set of CONV layers and make them trainable\n",
    "model = unfreeze_layer(base_model, model)\n",
    "# train the model again, this time fine-tuning *both* the final set\n",
    "# of CONV layers along with our set of FC layers\n",
    "H = train(model, config.UNFROZEN_NUM_EPOCHS)\n",
    "\n",
    "print(\"[INFO] evaluating after fine-tuning network...\")\n",
    "testGen.reset()\n",
    "predIdxs = model.predict(x=testGen,\n",
    "                         steps=(totalTest // config.BATCH_SIZE) + 1)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "print(classification_report(testGen.classes, predIdxs,\n",
    "                            target_names=testGen.class_indices.keys()))\n",
    "N = config.UNFROZEN_NUM_EPOCHS\n",
    "plot_training(H, 20, config.UNFROZEN_PLOT_PATH)\n",
    "\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] saving model...\")\n",
    "model.save(f'{config.MODEL_PATH}/keras')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c148e-d69e-4c15-a2f6-a2167c0a8b21",
   "metadata": {},
   "source": [
    "## Model Conversion to IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb40a7-ab22-4249-9702-cfa0fe3e4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino import convert_model\n",
    "import openvino\n",
    "import tensorflow as tf\n",
    "\n",
    "loaded_model =  tf.keras.models.load_model('model/')\n",
    "\n",
    "# The paths of the source and converted models\n",
    "\n",
    "ov_model = convert_model(loaded_model)\n",
    "# save model to OpenVINO IR for later use\n",
    "openvino.save_model(ov_model, 'model/ov model/model.xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc3d94-a42e-427e-85da-0b4445664ee7",
   "metadata": {},
   "source": [
    "## Model Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0a719-31e8-4656-82bc-9eafe78f59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openvino\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nncf\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow_datasets as tfds\n",
    "from openvino.runtime import Core\n",
    "from imutils import paths\n",
    "import config\n",
    "\n",
    "# Initialize the Inference Engine Core\n",
    "ie = Core()\n",
    "\n",
    "# Path to the XML file of the IR model (architecture)\n",
    "xml_file = 'model/ov model/model.xml'\n",
    "\n",
    "# Path to the bin file of the IR model (weights)\n",
    "bin_file = 'model/ov model/model.bin'\n",
    "\n",
    "# Load the IR model\n",
    "model = ie.read_model(model=xml_file, weights=bin_file)\n",
    "\n",
    "# Define a function to resize and normalize the images\n",
    "def preprocess_image(images):\n",
    "    # Resize the image to the desired shape (e.g., (224, 224))\n",
    "     # Ensure the image is a TensorFlow float32 tensor\n",
    "    image = images[\"image\"]\n",
    "    label = images[\"label\"]\n",
    "    image = tf.image.resize(image, (224, 224))  # Change the dimensions accordingly\n",
    "    # Normalize pixel values to the range [0, 1]\n",
    "    image = image / 255.0\n",
    "    image = tf.expand_dims(image, axis=0)  # Adds a new input channel at the last axis\n",
    "    label = tf.expand_dims(label, axis=0)  # Adds a new input channel at the last axis\n",
    "    dataset = {\"image\": image, \"label\": label}\n",
    "    return dataset\n",
    "\n",
    "val_loader = tfds.load(\"caltech101\", split=\"test\")\n",
    "val_loader = val_loader.map(preprocess_image)\n",
    "# Provide validation part of the dataset to collect statistics needed for the compression algorithm\n",
    "# Step 1: Initialize transformation function\n",
    "def transform_fn(data_item):\n",
    "    images = data_item[\"image\"]\n",
    "    print(images)\n",
    "    return images\n",
    "\n",
    "calibration_dataset = nncf.Dataset(val_loader, transform_fn)\n",
    "validation_dataset = nncf.Dataset(val_loader, transform_fn)\n",
    "\n",
    "def validate(model: openvino.runtime.CompiledModel,\n",
    "             validation_loader) -> float:\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    output = model.outputs[0]\n",
    "\n",
    "    for images in validation_loader:\n",
    "        pred = model(images[\"image\"])[output]\n",
    "        predictions.append(np.argmax(pred, axis=1))\n",
    "        references.append(images[\"label\"])\n",
    "        print(pred)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    references = np.concatenate(references, axis=0)\n",
    "    print(\"[INFO] Validation complete\")\n",
    "    return accuracy_score(predictions, references)\n",
    "\n",
    "\n",
    "\n",
    "print(\"[INFO] Quantizing model\")\n",
    "quantized_model = nncf.quantize_with_accuracy_control(model,\n",
    "                        calibration_dataset=calibration_dataset,\n",
    "                        validation_dataset=validation_dataset,\n",
    "                        validation_fn=validate,\n",
    "                        max_drop=0.01)\n",
    "print(quantized_model)\n",
    "print(\"[INFO] Saving quantized model\")\n",
    "openvino.serialize(quantized_model, \"model/quantized model/quantized_model.xml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04364f3c-3ae4-40f9-9ac6-5d17737e24cd",
   "metadata": {},
   "source": [
    "## Inference Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b03937-5115-41e0-b0a6-f64f4ef593d4",
   "metadata": {},
   "source": [
    "### Keras Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75af8075-642f-4098-9ae8-dda472c78101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# load the trained model from disk\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] loading model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m(config\u001b[38;5;241m.\u001b[39mMODEL_PATH)\n\u001b[1;32m     13\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference_test/images/image1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# our model was trained on RGB ordered images but OpenCV represents\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# images in BGR order, so swap the channels,\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "from openvino import Core\n",
    "import config\n",
    "\n",
    "# load the trained model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "model = load_model(config.MODEL_PATH)\n",
    "\n",
    "image = cv2.imread(filename=\"inference_test/images/image1.jpg\")\n",
    "# our model was trained on RGB ordered images but OpenCV represents\n",
    "# images in BGR order, so swap the channels,\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resize image\n",
    "image = cv2.resize(src=image, dsize=(224, 224))\n",
    "\n",
    "# convert the image to a floating point data type and perform mean\n",
    "# subtraction\n",
    "image = image.astype(\"float32\")\n",
    "mean = np.array([123.68, 116.779, 103.939][::-1], dtype=\"float32\")\n",
    "image -= mean\n",
    "\n",
    "result = model.predict(np.expand_dims(image, axis=0))[0]\n",
    "result_index = np.argmax(result)\n",
    "\n",
    "labels = config.CLASSES\n",
    "\n",
    "print(f\"The openvino model depicts {labels[result_index]}\")\n",
    "\n",
    "# draw the prediction on the output image\n",
    "text = \"IR model -> {}: {:.2f}%\".format(labels[result_index], result[result_index] * 100)\n",
    "cv2.putText(image, text, (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "            (0, 255, 0), 2)\n",
    "# show the resized_image image\n",
    "cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)\n",
    "print(\"Calculating IR Model Benchmarks\")\n",
    "\n",
    "\n",
    "print(\"Calculating WaverX-Vision in Keras format benchmarks\")\n",
    "num_images = 1000\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    compiled_model(inputs={input_layer: input_image})\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(\n",
    "    f\"Keras model in Inference Engine/CPU: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9b8ce2-ac13-4a19-8d9e-b5fb5602243c",
   "metadata": {},
   "source": [
    "### IR Models Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c6991e8-eda4-4668-b84d-b8b392c08dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model located at model/ov model\n",
      "Quantized model located at model/quantized model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@3602.668] global /workspace/suyue/aikit/opencv-python/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('inference_test/images/fire1.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) /workspace/suyue/aikit/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference_test/images/fire1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# our model was trained on RGB ordered images but OpenCV represents\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# images in BGR order, so swap the channels,\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Resize image to network input image shape\u001b[39;00m\n\u001b[1;32m     38\u001b[0m resized_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(src\u001b[38;5;241m=\u001b[39mimage, dsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.5) /workspace/suyue/aikit/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "from openvino import Core\n",
    "import config\n",
    "\n",
    "\n",
    "\n",
    "model_path = Path(\"model/ov model/\")\n",
    "model = Path(f\"{model_path}/model.xml\")\n",
    "weights = Path(f\"{model_path}/model.bin\")\n",
    "\n",
    "quantized_model_path = Path(\"model/quantized model/\")\n",
    "quantized_model = Path(f\"{model_path}/model.xml\")\n",
    "quantized_weights = Path(f\"{model_path}/model.bin\")\n",
    "print(\"Model located at {}\".format(model_path))\n",
    "print(\"Quantized model located at {}\".format(quantized_model_path))\n",
    "\n",
    "core = Core()\n",
    "read_model = core.read_model(model=model, weights=weights)\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\")\n",
    "read_quantized_model = core.read_model(model=quantized_model, weights=quantized_weights)\n",
    "compiled_quantized_model = core.compile_model(model=quantized_model, device_name=\"CPU\")\n",
    "\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "quantized_input_layer = compiled_quantized_model.input(0)\n",
    "quantized_output_layer = compiled_quantized_model.output(0)\n",
    "image = cv2.imread(filename=\"inference_test/images/fire1.jpg\")\n",
    "# our model was trained on RGB ordered images but OpenCV represents\n",
    "# images in BGR order, so swap the channels,\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resize image to network input image shape\n",
    "resized_image = cv2.resize(src=image, dsize=(224, 224))\n",
    "\n",
    "# Transpose image to network input shape\n",
    "input_image = np.expand_dims(np.transpose(resized_image, (1, 0, 2)), 0)\n",
    "\n",
    "result = compiled_model(inputs={input_layer: input_image})[output_layer][0]\n",
    "quantized_result = compiled_quantized_model(inputs={quantized_input_layer: input_image})[quantized_output_layer][0]\n",
    "result_index = np.argmax(result)\n",
    "quantized_result_index = np.argmax(quantized_result)\n",
    "\n",
    "labels = config.CLASSES\n",
    "\n",
    "print(f\"The openvino model depicts {labels[result_index]}\")\n",
    "# draw the prediction on the output image\n",
    "text = \"IR model -> {}: {:.2f}%\".format(labels[result_index], result[result_index] * 100)\n",
    "cv2.putText(resized_image, text, (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "            (0, 255, 0), 2)\n",
    "# show the resized_image image\n",
    "plt.imshow(\"Output\", resized_image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "print(f\"The openvino quantized model depicts {labels[quantized_result_index]}\")\n",
    "\n",
    "text = \"IR model -> {}: {:.2f}%\".format(labels[quantized_result_index], quantized_result[quantized_result_index] * 100)\n",
    "cv2.putText(resized_image, text, (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "            (0, 255, 0), 2)\n",
    "# show the resized_image image\n",
    "plt.imshow(\"Output\", resized_image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Calculating WaverX-Vision in IR format benchmarks\")\n",
    "num_images = 1000\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    compiled_model(inputs={input_layer: input_image})\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(\n",
    "    f\"IR model in Inference Engine/CPU: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")\n",
    "\n",
    "print(\"Calculating WaverX-Vision in Quantized IR format Benchmarks\")\n",
    "num_images = 1000\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    compiled_quantized_model(inputs={quantized_input_layer: input_image})\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(\n",
    "    f\"IR quantized model in Inference Engine/CPU: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443f9398-da68-453f-8fa4-793565653015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'inference_test/test_computer_vision_model/*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv inference_test/test_computer_vision_model/* inference_test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4a2990d-f99a-47d1-b5e9-2e52dfd86ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.0.0\n",
      "addict==2.4.0\n",
      "aiohttp @ file:///tmp/build/80754af9/aiohttp_1632748411861/work\n",
      "alembic @ file:///home/conda/feedstock_root/build_artifacts/alembic_1647367721563/work\n",
      "asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1660605382950/work\n",
      "astunparse==1.6.3\n",
      "async-timeout==3.0.1\n",
      "attrs @ file:///opt/conda/conda-bld/attrs_1642510447205/work\n",
      "backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work\n",
      "backports.functools-lru-cache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1618230623929/work\n",
      "bidict @ file:///tmp/build/80754af9/bidict_1607541094382/work\n",
      "blinker==1.4\n",
      "Bottleneck @ file:///tmp/build/80754af9/bottleneck_1648028898966/work\n",
      "brotlipy==0.7.0\n",
      "cachetools @ file:///tmp/build/80754af9/cachetools_1619597386817/work\n",
      "certifi==2022.9.14\n",
      "cffi @ file:///tmp/abs_98z5h56wf8/croots/recipe/cffi_1659598650955/work\n",
      "chardet @ file:///tmp/build/80754af9/chardet_1607706775000/work\n",
      "charset-normalizer==3.3.2\n",
      "click @ file:///tmp/build/80754af9/click_1646056590078/work\n",
      "cloudpickle @ file:///tmp/build/80754af9/cloudpickle_1598884132938/work\n",
      "contextlib2==0.5.5\n",
      "cryptography @ file:///tmp/build/80754af9/cryptography_1652101588893/work\n",
      "cycler @ file:///tmp/build/80754af9/cycler_1637851556182/work\n",
      "Cython==0.29.25\n",
      "debugpy @ file:///home/conda/feedstock_root/build_artifacts/debugpy_1660619001967/work\n",
      "decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work\n",
      "entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work\n",
      "executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1661768596365/work\n",
      "Flask @ file:///home/ktietz/src/ci/flask_1611932660458/work\n",
      "Flask-Cors @ file:///tmp/build/80754af9/flask-cors_1609957731919/work\n",
      "Flask-SocketIO @ file:///tmp/build/80754af9/flask-socketio_1622558327629/work\n",
      "flatbuffers @ file:///tmp/build/80754af9/python-flatbuffers_1614345733764/work\n",
      "fonttools==4.25.0\n",
      "future @ file:///tmp/build/80754af9/future_1607571303524/work\n",
      "gast==0.5.4\n",
      "gevent @ file:///tmp/build/80754af9/gevent_1628273270105/work\n",
      "gevent-websocket @ file:///tmp/build/80754af9/gevent-websocket_1614094053512/work\n",
      "google-auth==2.23.4\n",
      "google-auth-oauthlib==1.0.0\n",
      "google-pasta==0.2.0\n",
      "greenlet @ file:///tmp/build/80754af9/greenlet_1628888132713/work\n",
      "grpcio==1.59.2\n",
      "h5py @ file:///tmp/build/80754af9/h5py_1637138488546/work\n",
      "hyperopt @ file:///home/conda/feedstock_root/build_artifacts/hyperopt_1602661729544/work\n",
      "idna @ file:///home/linux1/recipes/ci/idna_1610986105248/work\n",
      "imageio==2.32.0\n",
      "importlib-metadata @ file:///tmp/build/80754af9/importlib-metadata_1631916692253/work\n",
      "importlib-resources @ file:///tmp/build/80754af9/importlib_resources_1625135880749/work\n",
      "imutil==0.3.4\n",
      "intel_tensorflow @ file:///home/whls/intel_tensorflow-2.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "ipykernel @ file:///home/conda/feedstock_root/build_artifacts/ipykernel_1663104864651/work\n",
      "ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1662481517711/work\n",
      "itsdangerous @ file:///tmp/build/80754af9/itsdangerous_1621432558163/work\n",
      "jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1659959867326/work\n",
      "Jinja2 @ file:///tmp/build/80754af9/jinja2_1624781299557/work\n",
      "joblib==1.3.2\n",
      "jupyter-client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1633454794268/work\n",
      "jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1658332352542/work\n",
      "keras==2.14.0\n",
      "Keras-Preprocessing @ file:///tmp/build/80754af9/keras-preprocessing_1612283640596/work\n",
      "kernda==0.3.0\n",
      "kiwisolver @ file:///opt/conda/conda-bld/kiwisolver_1638569886207/work\n",
      "lazy_loader==0.3\n",
      "libclang==16.0.6\n",
      "Mako @ file:///tmp/build/80754af9/mako_1610650590832/work\n",
      "Markdown @ file:///tmp/build/80754af9/markdown_1614363852612/work\n",
      "MarkupSafe @ file:///tmp/build/80754af9/markupsafe_1621523467000/work\n",
      "matplotlib @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-suite_1659031536286/work\n",
      "matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work\n",
      "mkl-fft==1.3.1\n",
      "mkl-random==1.2.2\n",
      "mkl-service==2.4.0\n",
      "mkl-umath==0.1.1\n",
      "ml-dtypes==0.2.0\n",
      "multidict @ file:///tmp/build/80754af9/multidict_1607367781142/work\n",
      "munkres==1.1.4\n",
      "nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1648959695634/work\n",
      "networkx==3.2.1\n",
      "neural-compressor-full @ file:///root/w0/workspace/lpot-release-wheel-build/lpot-models/dist/neural_compressor_full-1.13-py3-none-any.whl\n",
      "numexpr==2.8.1\n",
      "numpy==1.26.1\n",
      "oauthlib @ file:///home/conda/feedstock_root/build_artifacts/oauthlib_1643507977997/work\n",
      "opencv-python-headless==4.8.1.78\n",
      "openvino==2023.1.0\n",
      "openvino-telemetry==2023.2.1\n",
      "opt-einsum @ file:///tmp/build/80754af9/opt_einsum_1621500238896/work\n",
      "ovmsclient==2023.1\n",
      "packaging @ file:///tmp/build/80754af9/packaging_1637314298585/work\n",
      "pandas==1.4.1\n",
      "parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work\n",
      "pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1602535608087/work\n",
      "pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work\n",
      "Pillow==9.2.0\n",
      "prettytable==0.7.2\n",
      "prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1662384672173/work\n",
      "protobuf==4.25.0\n",
      "psutil @ file:///tmp/build/80754af9/psutil_1612297992929/work\n",
      "ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\n",
      "pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work\n",
      "py-cpuinfo @ file:///tmp/build/80754af9/py-cpuinfo_1618437357364/work\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycocotools @ file:///home/conda/feedstock_root/build_artifacts/pycocotools_1626785515988/work\n",
      "pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work\n",
      "Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1660666458521/work\n",
      "PyJWT @ file:///home/conda/feedstock_root/build_artifacts/pyjwt_1638819640841/work\n",
      "pymongo==3.12.0\n",
      "pyOpenSSL @ file:///opt/conda/conda-bld/pyopenssl_1643788558760/work\n",
      "pyparsing @ file:///opt/conda/conda-bld/pyparsing_1661452539315/work\n",
      "PyQt5==5.12.3\n",
      "PyQt5_sip==4.19.18\n",
      "PyQtChart==5.12\n",
      "PyQtWebEngine==5.12.1\n",
      "PySocks @ file:///tmp/build/80754af9/pysocks_1605305812635/work\n",
      "python-dateutil==2.8.2\n",
      "python-engineio @ file:///tmp/build/80754af9/python-engineio_1618814346232/work\n",
      "python-socketio @ file:///tmp/build/80754af9/python-socketio_1624540179952/work\n",
      "pytz @ file:///opt/conda/conda-bld/pytz_1654762638606/work\n",
      "PyYAML==6.0\n",
      "pyzmq @ file:///home/conda/feedstock_root/build_artifacts/pyzmq_1663830512540/work\n",
      "requests==2.31.0\n",
      "requests-oauthlib==1.3.0\n",
      "rsa @ file:///tmp/build/80754af9/rsa_1614366226499/work\n",
      "schema @ file:///home/conda/feedstock_root/build_artifacts/schema_1612181599899/work\n",
      "scikit-image==0.22.0\n",
      "scikit-learn==1.3.1\n",
      "scipy==1.11.3\n",
      "sigopt @ file:///home/conda/feedstock_root/build_artifacts/sigopt_1591809841849/work\n",
      "six @ file:///tmp/build/80754af9/six_1644875935023/work\n",
      "SQLAlchemy @ file:///tmp/build/80754af9/sqlalchemy_1638277513377/work\n",
      "stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1661624565828/work\n",
      "TBB==0.2\n",
      "tensorboard==2.14.1\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorboard-plugin-wit==1.6.0\n",
      "tensorflow==2.14.0\n",
      "tensorflow-estimator==2.14.0\n",
      "tensorflow-io-gcs-filesystem==0.34.0\n",
      "termcolor==1.1.0\n",
      "threadpoolctl @ file:///Users/ktietz/demo/mc3/conda-bld/threadpoolctl_1629802263681/work\n",
      "tifffile==2023.9.26\n",
      "tornado @ file:///tmp/build/80754af9/tornado_1606942317143/work\n",
      "tqdm @ file:///tmp/build/80754af9/tqdm_1635330843403/work\n",
      "traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1663005918942/work\n",
      "typing_extensions @ file:///opt/conda/conda-bld/typing_extensions_1647553014482/work\n",
      "urllib3 @ file:///tmp/abs_5dhwnz6atv/croots/recipe/urllib3_1659110457909/work\n",
      "wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1600965781394/work\n",
      "Werkzeug @ file:///tmp/build/80754af9/werkzeug_1635505089296/work\n",
      "wrapt @ file:///tmp/build/80754af9/wrapt_1638433857881/work\n",
      "yarl @ file:///tmp/build/80754af9/yarl_1606939947528/work\n",
      "zipp @ file:///tmp/build/80754af9/zipp_1633618647012/work\n",
      "zope.event==4.5.0\n",
      "zope.interface @ file:///home/conda/feedstock_root/build_artifacts/zope.interface_1618486022845/work\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f66b5e0f-2f61-4c1a-bb7f-da58dc1da9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: opencv-python 4.8.1.78\n",
      "Uninstalling opencv-python-4.8.1.78:\n",
      "  Successfully uninstalled opencv-python-4.8.1.78\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall opencv-python==4.8.1.78 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c33073a9-5062-4be2-adc6-afa643cde109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\n",
      "   KMP_BLOCKTIME=1\n",
      "   KMP_SETTINGS=1\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=128\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=1\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_ENABLE_TASK_THROTTLING=true\n",
      "   KMP_FORCE_MONOTONIC_DYNAMIC_SCHEDULE=false\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_FORKJOIN_FRAMES=true\n",
      "   KMP_FORKJOIN_FRAMES_MODE=3\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_ITT_PREPARE_DELAY=0\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_MWAIT_HINTS=0\n",
      "   KMP_NESTING_MODE=0\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=8M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_PROC_BIND=spread\n",
      "   KMP_TEAMS_THREAD_LIMIT=8\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_TPAUSE=0\n",
      "   KMP_USER_LEVEL_MWAIT=false\n",
      "   KMP_USE_YIELD=1\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=true\n",
      "   LIBOMP_NUM_HIDDEN_HELPER_THREADS=8\n",
      "   LIBOMP_USE_HIDDEN_HELPER_TASK=true\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %i thread %n bound to OS proc set {%A}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=1\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED: deprecated; max-active-levels-var=1\n",
      "   OMP_NUM_TEAMS=0\n",
      "   OMP_NUM_THREADS: value is not defined\n",
      "   OMP_PLACES='threads'\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=8M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_TEAMS_THREAD_LIMIT=0\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_TOOL=enabled\n",
      "   OMP_TOOL_LIBRARIES: value is not defined\n",
      "   OMP_TOOL_VERBOSE_INIT: value is not defined\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='verbose,warnings,respect,granularity=thread,compact,1,0'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src/waverX-Vision\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc62a9f-58b8-45d8-85d3-1919121d3951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intel TensorFlow & Quantization",
   "language": "python",
   "name": "oneapi-aikit-dlpackage-with-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
