{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d501a1-4df7-4048-8169-437655a74322",
   "metadata": {},
   "source": [
    "# Climate Wavers Vision Model: WaverX-Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfc9e8c-8f3d-41cb-b4e4-0868d576e8fe",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f5c2fc-a07e-480c-93fe-73bd45bc76d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m291.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.26.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m281.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting imutil\n",
      "  Downloading imutil-0.3.4-py2.py3-none-any.whl (208 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.8/208.8 kB\u001b[0m \u001b[31m327.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opencv-python\n",
      "  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m295.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ovmsclient==2023.1\n",
      "  Downloading ovmsclient-2023.1-py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.2/146.2 kB\u001b[0m \u001b[31m278.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m302.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn==1.3.1\n",
      "  Downloading scikit_learn-1.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m242.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting openvino==2023.1.0\n",
      "  Downloading openvino-2023.1.0-12185-cp39-cp39-manylinux2014_x86_64.whl (35.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m274.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow\n",
      "  Downloading tensorflow-2.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m296.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting ipython\n",
      "  Downloading ipython-8.17.2-py3-none-any.whl (808 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.4/808.4 kB\u001b[0m \u001b[31m359.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting addict\n",
      "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.4 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ovmsclient==2023.1->-r requirements.txt (line 5)) (4.25.0)\n",
      "Requirement already satisfied: requests>=2.27.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ovmsclient==2023.1->-r requirements.txt (line 5)) (2.31.0)\n",
      "Requirement already satisfied: grpcio>=1.47.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ovmsclient==2023.1->-r requirements.txt (line 5)) (1.59.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from scikit-learn==1.3.1->-r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from scikit-learn==1.3.1->-r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from scikit-learn==1.3.1->-r requirements.txt (line 7)) (1.11.3)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.1.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from openvino==2023.1.0->-r requirements.txt (line 8)) (2023.2.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2022.1)\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.22.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m298.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from imutil->-r requirements.txt (line 3)) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (5.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (2.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (20210226132247)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (2.14.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (0.5.4)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (2.14.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (3.6.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (16.0.6)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (0.34.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (2.14.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (4.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (58.0.4)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 9)) (1.13.3)\n",
      "Requirement already satisfied: exceptiongroup in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (1.1.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (3.0.31)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (0.1.6)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (2.13.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (4.8.0)\n",
      "Requirement already satisfied: decorator in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (5.1.1)\n",
      "Requirement already satisfied: stack-data in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (0.5.0)\n",
      "Requirement already satisfied: traitlets>=5 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (5.4.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from ipython->-r requirements.txt (line 10)) (0.18.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 9)) (0.37.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from jedi>=0.16->ipython->-r requirements.txt (line 10)) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from pexpect>4.3->ipython->-r requirements.txt (line 10)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->-r requirements.txt (line 10)) (0.2.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from requests>=2.27.1->ovmsclient==2023.1->-r requirements.txt (line 5)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from requests>=2.27.1->ovmsclient==2023.1->-r requirements.txt (line 5)) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from requests>=2.27.1->ovmsclient==2023.1->-r requirements.txt (line 5)) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from requests>=2.27.1->ovmsclient==2023.1->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (0.7.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (2.23.4)\n",
      "Collecting networkx>=2.8\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m326.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tifffile>=2022.8.12\n",
      "  Downloading tifffile-2023.9.26-py3-none-any.whl (222 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.9/222.9 kB\u001b[0m \u001b[31m312.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lazy_loader>=0.3\n",
      "  Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Collecting imageio>=2.27\n",
      "  Downloading imageio-2.32.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.3/313.3 kB\u001b[0m \u001b[31m338.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: executing in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from stack-data->ipython->-r requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: pure-eval in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from stack-data->ipython->-r requirements.txt (line 10)) (0.2.2)\n",
      "Requirement already satisfied: asttokens in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from stack-data->ipython->-r requirements.txt (line 10)) (2.0.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->-r requirements.txt (line 9)) (3.2.0)\n",
      "Installing collected packages: addict, numpy, networkx, lazy_loader, tifffile, pandas, ovmsclient, openvino, opencv-python, imageio, scikit-learn, scikit-image, matplotlib, ipython, imutil, tensorflow\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 2.6.2\n",
      "    Uninstalling networkx-2.6.2:\n",
      "      Successfully uninstalled networkx-2.6.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "intel-tensorflow 2.9.1 requires flatbuffers<2,>=1.12, but you have flatbuffers 20210226132247 which is incompatible.\n",
      "intel-tensorflow 2.9.1 requires gast<=0.4.0,>=0.2.1, but you have gast 0.5.4 which is incompatible.\n",
      "intel-tensorflow 2.9.1 requires keras<2.10.0,>=2.9.0rc0, but you have keras 2.14.0 which is incompatible.\n",
      "intel-tensorflow 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.0 which is incompatible.\n",
      "intel-tensorflow 2.9.1 requires tensorboard<2.10,>=2.9, but you have tensorboard 2.14.1 which is incompatible.\n",
      "intel-tensorflow 2.9.1 requires tensorflow-estimator<2.10.0,>=2.9.0rc0, but you have tensorflow-estimator 2.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed addict-2.4.0 imageio-2.32.0 imutil-0.3.4 ipython-8.17.2 lazy_loader-0.3 matplotlib-3.8.1 networkx-3.2.1 numpy-1.26.1 opencv-python-4.8.1.78 openvino-2023.1.0 ovmsclient-2023.1 pandas-2.1.2 scikit-image-0.22.0 scikit-learn-1.3.1 tensorflow-2.14.0 tifffile-2023.9.26\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a44f8d-fd36-4b5f-bdde-ff4b2a56ca3e",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529dad9-2c46-4d58-9629-3af1d4bb55b8",
   "metadata": {},
   "source": [
    "### Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a31375-8fdc-4c37-927a-1201b8c319ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import os\n",
    "# initialize the path to the *original* input directory of images\n",
    "INPUT_DATASET = \"dataset\"\n",
    "# initialize the base path to the *new* directory that will contain\n",
    "# our images after computing the training and testing split\n",
    "BASE_PATH = \"model_dataset\"\n",
    "# derive the training, validation, and testing directories\n",
    "TRAIN_PATH = os.path.sep.join([BASE_PATH, \"training\"])\n",
    "VAL_PATH = os.path.sep.join([BASE_PATH, \"validation\"])\n",
    "TEST_PATH = os.path.sep.join([BASE_PATH, \"testing\"])\n",
    "\n",
    "# define the amount of data that will be used training\n",
    "TRAIN_SPLIT = 0.75\n",
    "# the amount of validation data will be a percentage of the\n",
    "# *training* data\n",
    "VAL_SPLIT = 0.1\n",
    "# define the names of the classes\n",
    "CLASSES = [\"Earthquake\", \"Drought\",\n",
    "           \"Damaged Infrastructure\", \"Human Damage\", \"Human\", \"Land Slide\", \"Non Damage Buildings and  Street\", \"Non Damage Wildlife Forest\",\n",
    "           \"Sea\", \"Urban Fire\", \"Wild Fire\", \"Water Disaster\"]\n",
    "\n",
    "CLASSES.sort()\n",
    "\n",
    "# initialize the initial learning rate, batch size, and number of\n",
    "# epochs to train for\n",
    "INIT_LR = 1e-4\n",
    "BS = 32\n",
    "NUM_EPOCHS = 50\n",
    "UNFROZEN_NUM_EPOCHS = 20\n",
    "# define the path to the serialized output model after training\n",
    "MODEL_PATH = \"model\"\n",
    "# define the path to the output training history plots\n",
    "UNFROZEN_PLOT_PATH = os.path.sep.join([\"output\", \"plots/unfrozen.png\"])\n",
    "WARMUP_PLOT_PATH = os.path.sep.join([\"output\", \"plots/warmup.png\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a269fb8-58a9-4c8e-8b73-d2bd7af805c7",
   "metadata": {},
   "source": [
    "### Dataset Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afc038d7-c88a-4431-8d9d-37dc215873b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building 'training' split\n",
      "[INFO] building 'validation' split\n",
      "[INFO] building 'testing' split\n",
      "[INFO] Total training data === 12428\n",
      "[INFO] Total valid training data === 12428\n",
      "[INFO] Total validating data === 1968\n",
      "[INFO] Total valid validating data === 1968\n",
      "[INFO] Total testing data === 5913\n",
      "[INFO] Total valid testing data === 5913\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import config\n",
    "from imutils import paths\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#grab the paths to all input images in the original input directory\n",
    "# and shuffle them\n",
    "imagePaths = list(paths.list_images(config.INPUT_DATASET))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "# compute the training and testing split\n",
    "index = int(len(imagePaths) * config.TRAIN_SPLIT)\n",
    "trainPaths = imagePaths[:index]\n",
    "testPaths = imagePaths[index:]\n",
    "# we'll be using part of the training data for validation\n",
    "index = int(len(trainPaths) * config.VAL_SPLIT)\n",
    "valPaths = trainPaths[:index]\n",
    "trainPaths = trainPaths[index:]\n",
    "# define the datasets that we'll be building\n",
    "datasets = [\n",
    "    (\"training\", trainPaths, config.TRAIN_PATH),\n",
    "    (\"validation\", valPaths, config.VAL_PATH),\n",
    "    (\"testing\", testPaths, config.TEST_PATH)\n",
    "]\n",
    "\n",
    "# loop over the datasets\n",
    "for (dType, imagePaths, baseOutput) in datasets:\n",
    "    # show which data split we are creating\n",
    "    print(\"[INFO] building '{}' split\".format(dType))\n",
    "    # if the output base output directory does not exist, create it\n",
    "    if not os.path.exists(baseOutput):\n",
    "        print(\"[INFO] 'creating {}' directory\".format(baseOutput))\n",
    "        os.makedirs(baseOutput)\n",
    "    # loop over the input image paths\n",
    "    for inputPath in imagePaths:\n",
    "        # extract the filename of the input image along with its\n",
    "        # corresponding class label\n",
    "        filename = inputPath.split(os.path.sep)[-1]\n",
    "        label = inputPath.split(os.path.sep)[-2]\n",
    "        # build the path to the label directory\n",
    "        labelPath = os.path.sep.join([baseOutput, label])\n",
    "        # if the label output directory does not exist, create it\n",
    "        if not os.path.exists(labelPath):\n",
    "            print(\"[INFO] 'creating {}' directory\".format(labelPath))\n",
    "            os.makedirs(labelPath)\n",
    "        # construct the path to the destination image and then copy\n",
    "        # the image itself\n",
    "        p = os.path.sep.join([labelPath, filename])\n",
    "        shutil.copy2(inputPath, p)\n",
    "\n",
    "# Total number of image paths in training, validation,\n",
    "# and testing directories\n",
    "totalTrain = len(list(paths.list_images(config.TRAIN_PATH)))\n",
    "totalVal = len(list(paths.list_images(config.VAL_PATH)))\n",
    "totalTest = len(list(paths.list_images(config.TEST_PATH)))\n",
    "\n",
    "\n",
    "def filter_valid_images(directory):\n",
    "    valid_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        for file in os.listdir(file_path):\n",
    "            try:\n",
    "                # Attempt to open the file as an image\n",
    "                Image.open(f'{file_path}/{file}')\n",
    "                current_time = datetime.now()\n",
    "                formatted_time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                valid_files.append(file)\n",
    "                # Log results\n",
    "                with open('dataset_filtering.log', 'a') as log:\n",
    "                    log.write(f'{file} is valid -[{formatted_time}]\\n')\n",
    "            except OSError as e:\n",
    "                # Handle exceptions and log the error\n",
    "                image_path = os.path.join(file_path, file)\n",
    "                current_time = datetime.now()\n",
    "                formatted_time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                os.remove(image_path)\n",
    "                with open('dataset_filtering.log', 'w') as log:\n",
    "                    log.write(f\"Skipping file {file}: {e} -[{formatted_time}]\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle other exceptions\n",
    "                image_path = os.path.join(file_path, file)\n",
    "                current_time = datetime.now()\n",
    "                formatted_time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                os.remove(image_path)\n",
    "                with open('dataset_filtering.log', 'w') as log:\n",
    "                    log.write(f\"Unexpected error: {e} -[{formatted_time}]\\n\")\n",
    "    return len(valid_files)\n",
    "\n",
    "valid_train = filter_valid_images(config.TRAIN_PATH)\n",
    "print(\"[INFO] Total training data === {}\".format(totalTrain))\n",
    "print(\"[INFO] Total valid training data === {}\".format(valid_train))\n",
    "valid_val = filter_valid_images(config.VAL_PATH)\n",
    "print(\"[INFO] Total validating data === {}\".format(totalVal))\n",
    "print(\"[INFO] Total valid validating data === {}\".format(valid_val))\n",
    "valid_test = filter_valid_images(config.TEST_PATH)\n",
    "print(\"[INFO] Total testing data === {}\".format(totalTest))\n",
    "print(\"[INFO] Total valid testing data === {}\".format(valid_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d13fc-87cf-4fe7-9d0a-d1dd4f70a6ba",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2cbbb5e7-e463-4342-ad73-00c406f6fcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plots/warmup.png\n",
      "Total training data === 12428\n",
      "Total validating data === 1968\n",
      "Total testing data === 5913\n",
      "Found 12428 images belonging to 12 classes.\n",
      "Found 1968 images belonging to 12 classes.\n",
      "Found 5913 images belonging to 12 classes.\n",
      "[INFO] preparing model...\n",
      "[INFO] compiling model...\n",
      "[INFO] training model...\n",
      "Epoch 1/50\n",
      "136/388 [=========>....................] - ETA: 13:48 - loss: 0.2530 - accuracy: 0.4858"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 165\u001b[0m\n\u001b[1;32m    161\u001b[0m model \u001b[38;5;241m=\u001b[39m new_layer(base_model)\n\u001b[1;32m    162\u001b[0m model \u001b[38;5;241m=\u001b[39m freeze_layer(model, base_model)\n\u001b[0;32m--> 165\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# reset the testing generator and then use our trained model to\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# make predictions on the data\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] evaluating network...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[47], line 82\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, epochs):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] training model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m H\n",
      "File \u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-TF/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report\n",
    "import config\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "LAYERS_TO_FREEZE = 172\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
    "                help=\"path to output loss/accuracy plot\")\n",
    "args = vars(ap.parse_args(args=[]))\n",
    "# Total number of image paths in training, validation,\n",
    "# and testing directories\n",
    "total_train = len(list(paths.list_images(config.TRAIN_PATH)))\n",
    "total_val = len(list(paths.list_images(config.VAL_PATH)))\n",
    "total_test = len(list(paths.list_images(config.TEST_PATH)))\n",
    "\n",
    "\n",
    "def freeze_layer(model, base_model):\n",
    "    \"\"\"Freeze all layers and compile the model\"\"\"\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    opt = Adam(learning_rate=config.INIT_LR, decay=config.INIT_LR / config.NUM_EPOCHS)\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def new_layer(base_model):\n",
    "    # construct the new layer of the model that will be placed on top of the\n",
    "    # the base model\n",
    "    top_model = base_model.output\n",
    "    top_model = AveragePooling2D(pool_size=(7, 7))(top_model)\n",
    "    top_model = Flatten(name=\"flatten\")(top_model)\n",
    "    top_model = Dense(256, activation=\"relu\")(top_model)\n",
    "    top_model = Dropout(0.5)(top_model)\n",
    "    top_model = Dense(len(config.CLASSES), activation=\"softmax\")(top_model)\n",
    "    model = Model(inputs=base_model.input, outputs=top_model)\n",
    "    return model\n",
    "\n",
    "def unfreeze_layer(baseModel, model):\n",
    "    for layer in baseModel.layers[15:]:\n",
    "        layer.trainable = True\n",
    "    # loop over the layers in the model and show which ones are trainable\n",
    "    # or not\n",
    "    for layer in baseModel.layers:\n",
    "        print(\"{}: {}\".format(layer, layer.trainable))\n",
    "    # for the changes to the model to take affect we need to recompile\n",
    "    # the model, this time using SGD with a *very* small learning rate\n",
    "    print(\"[INFO] re-compiling model...\")\n",
    "    opt = ADAM(learning_rate=config.INIT_LR, decay=config.INIT_LR / config.UNFROZEN_NUM_EPOCHS)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def train_model(model, epochs):\n",
    "    # train the model\n",
    "    print(\"[INFO] training model...\")\n",
    "    H = model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=total_train // config.BS,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=total_val // config.BS,\n",
    "        epochs=epochs)\n",
    "\n",
    "    return H\n",
    "\n",
    "def plot_training(H, N, plot_path):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(plot_path)\n",
    "\n",
    "print(\"Total training data === {}\".format(total_train))\n",
    "print(\"Total validating data === {}\".format(total_val))\n",
    "print(\"Total testing data === {}\".format(total_test))\n",
    "\n",
    "# initialize the training training data augmentation object\n",
    "train_aug = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")\n",
    "# initialize the validation/testing data augmentation object (which\n",
    "# we'll be adding mean subtraction to)\n",
    "val_aug = ImageDataGenerator()\n",
    "# define the ImageNet mean subtraction (in RGB order) and set the\n",
    "# the mean subtraction value for each of the data augmentation\n",
    "# objects\n",
    "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
    "train_aug.mean = mean\n",
    "val_aug.mean = mean\n",
    "\n",
    "\n",
    "# initialize the training generator\n",
    "train_gen = train_aug.flow_from_directory(\n",
    "    config.TRAIN_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    batch_size=config.BS)\n",
    "# initialize the validation generator\n",
    "val_gen = val_aug.flow_from_directory(\n",
    "    config.VAL_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=config.BS)\n",
    "# initialize the testing generator\n",
    "test_gen = val_aug.flow_from_directory(\n",
    "    config.TEST_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=config.BS)\n",
    "\n",
    "\n",
    "# off\n",
    "print(\"[INFO] preparing model...\")\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False,\n",
    "                     input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# place the top FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = new_layer(base_model)\n",
    "model = freeze_layer(model, base_model)\n",
    "\n",
    "\n",
    "H = train_model(model, config.NUM_EPOCHS)\n",
    "# reset the testing generator and then use our trained model to\n",
    "# make predictions on the data\n",
    "print(\"[INFO] evaluating network...\")\n",
    "test_gen.reset()\n",
    "predIdxs = model.predict(test_gen, steps=(total_test // config.BS) + 1)\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(test_gen.classes, predIdxs,\n",
    "                            target_names=test_gen.class_indices.keys()))\n",
    "N = config.NUM_EPOCHS\n",
    "plot_training(H, N, config.WARMUP_PLOT_PATH)\n",
    "\n",
    "# reset our data generators\n",
    "train_gen.reset()\n",
    "val_gen.reset()\n",
    "# now that the head FC layers have been trained/initialized, lets\n",
    "# unfreeze the final set of CONV layers and make them trainable\n",
    "model = unfreeze_layer(base_model, model)\n",
    "# train the model again, this time fine-tuning *both* the final set\n",
    "# of CONV layers along with our set of FC layers\n",
    "H = train(model, config.UNFROZEN_NUM_EPOCHS)\n",
    "\n",
    "print(\"[INFO] evaluating after fine-tuning network...\")\n",
    "testGen.reset()\n",
    "predIdxs = model.predict(x=testGen,\n",
    "                         steps=(totalTest // config.BATCH_SIZE) + 1)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "print(classification_report(testGen.classes, predIdxs,\n",
    "                            target_names=testGen.class_indices.keys()))\n",
    "N = config.UNFROZEN_NUM_EPOCHS\n",
    "plot_training(H, 20, config.UNFROZEN_PLOT_PATH)\n",
    "\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] saving model...\")\n",
    "model.save(f'{config.MODEL_PATH}/keras')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c148e-d69e-4c15-a2f6-a2167c0a8b21",
   "metadata": {},
   "source": [
    "## Model Conversion to IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb40a7-ab22-4249-9702-cfa0fe3e4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino import convert_model\n",
    "import openvino\n",
    "import tensorflow as tf\n",
    "\n",
    "loaded_model =  tf.keras.models.load_model('model/')\n",
    "\n",
    "# The paths of the source and converted models\n",
    "\n",
    "ov_model = convert_model(loaded_model)\n",
    "# save model to OpenVINO IR for later use\n",
    "openvino.save_model(ov_model, 'model/ov model/model.xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc3d94-a42e-427e-85da-0b4445664ee7",
   "metadata": {},
   "source": [
    "## Model Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0a719-31e8-4656-82bc-9eafe78f59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openvino\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nncf\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow_datasets as tfds\n",
    "from openvino.runtime import Core\n",
    "from imutils import paths\n",
    "import config\n",
    "\n",
    "# Initialize the Inference Engine Core\n",
    "ie = Core()\n",
    "\n",
    "# Path to the XML file of the IR model (architecture)\n",
    "xml_file = 'model/ov model/model.xml'\n",
    "\n",
    "# Path to the bin file of the IR model (weights)\n",
    "bin_file = 'model/ov model/model.bin'\n",
    "\n",
    "# Load the IR model\n",
    "model = ie.read_model(model=xml_file, weights=bin_file)\n",
    "\n",
    "# Define a function to resize and normalize the images\n",
    "def preprocess_image(images):\n",
    "    # Resize the image to the desired shape (e.g., (224, 224))\n",
    "     # Ensure the image is a TensorFlow float32 tensor\n",
    "    image = images[\"image\"]\n",
    "    label = images[\"label\"]\n",
    "    image = tf.image.resize(image, (224, 224))  # Change the dimensions accordingly\n",
    "    # Normalize pixel values to the range [0, 1]\n",
    "    image = image / 255.0\n",
    "    image = tf.expand_dims(image, axis=0)  # Adds a new input channel at the last axis\n",
    "    label = tf.expand_dims(label, axis=0)  # Adds a new input channel at the last axis\n",
    "    dataset = {\"image\": image, \"label\": label}\n",
    "    return dataset\n",
    "\n",
    "val_loader = tfds.load(\"caltech101\", split=\"test\")\n",
    "val_loader = val_loader.map(preprocess_image)\n",
    "# Provide validation part of the dataset to collect statistics needed for the compression algorithm\n",
    "# Step 1: Initialize transformation function\n",
    "def transform_fn(data_item):\n",
    "    images = data_item[\"image\"]\n",
    "    print(images)\n",
    "    return images\n",
    "\n",
    "calibration_dataset = nncf.Dataset(val_loader, transform_fn)\n",
    "validation_dataset = nncf.Dataset(val_loader, transform_fn)\n",
    "\n",
    "def validate(model: openvino.runtime.CompiledModel,\n",
    "             validation_loader) -> float:\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    output = model.outputs[0]\n",
    "\n",
    "    for images in validation_loader:\n",
    "        pred = model(images[\"image\"])[output]\n",
    "        predictions.append(np.argmax(pred, axis=1))\n",
    "        references.append(images[\"label\"])\n",
    "        print(pred)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    references = np.concatenate(references, axis=0)\n",
    "    print(\"[INFO] Validation complete\")\n",
    "    return accuracy_score(predictions, references)\n",
    "\n",
    "\n",
    "\n",
    "print(\"[INFO] Quantizing model\")\n",
    "quantized_model = nncf.quantize_with_accuracy_control(model,\n",
    "                        calibration_dataset=calibration_dataset,\n",
    "                        validation_dataset=validation_dataset,\n",
    "                        validation_fn=validate,\n",
    "                        max_drop=0.01)\n",
    "print(quantized_model)\n",
    "print(\"[INFO] Saving quantized model\")\n",
    "openvino.serialize(quantized_model, \"model/quantized model/quantized_model.xml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04364f3c-3ae4-40f9-9ac6-5d17737e24cd",
   "metadata": {},
   "source": [
    "## Inference Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b03937-5115-41e0-b0a6-f64f4ef593d4",
   "metadata": {},
   "source": [
    "### Keras Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af8075-642f-4098-9ae8-dda472c78101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "from openvino import Core\n",
    "import config\n",
    "\n",
    "# load the trained model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "model = load_model(config.MODEL_PATH)\n",
    "\n",
    "image = cv2.imread(filename=\"inference_test/images/image1.jpg\")\n",
    "# our model was trained on RGB ordered images but OpenCV represents\n",
    "# images in BGR order, so swap the channels,\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resize image\n",
    "image = cv2.resize(src=image, dsize=(224, 224))\n",
    "\n",
    "# convert the image to a floating point data type and perform mean\n",
    "# subtraction\n",
    "image = image.astype(\"float32\")\n",
    "mean = np.array([123.68, 116.779, 103.939][::-1], dtype=\"float32\")\n",
    "image -= mean\n",
    "\n",
    "result = model.predict(np.expand_dims(image, axis=0))[0]\n",
    "result_index = np.argmax(result)\n",
    "\n",
    "labels = config.CLASSES\n",
    "\n",
    "print(f\"The openvino model depicts {labels[result_index]}\")\n",
    "\n",
    "# draw the prediction on the output image\n",
    "text = \"IR model -> {}: {:.2f}%\".format(labels[result_index], result[result_index] * 100)\n",
    "cv2.putText(image, text, (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "            (0, 255, 0), 2)\n",
    "# show the resized_image image\n",
    "cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)print(\"Calculating IR Model Benchmarks\")\n",
    "\n",
    "\n",
    "print(\"Calculating WaverX-Vision in Keras format benchmarks\")\n",
    "num_images = 1000\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    compiled_model(inputs={input_layer: input_image})\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(\n",
    "    f\"Keras model in Inference Engine/CPU: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9b8ce2-ac13-4a19-8d9e-b5fb5602243c",
   "metadata": {},
   "source": [
    "### IR Models Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6991e8-eda4-4668-b84d-b8b392c08dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "from openvino import Core\n",
    "import config\n",
    "\n",
    "\n",
    "\n",
    "model_path = Path(\"computer_vision_model/model/ov model/\")\n",
    "model = Path(f\"{model_path}/model.xml\")\n",
    "weights = Path(f\"{model_path}/model.bin\")\n",
    "\n",
    "quantized_model_path = Path(\"computer_vision_model/model/quantized model/\")\n",
    "quantized_model = Path(f\"{model_path}/model.xml\")\n",
    "quantized_weights = Path(f\"{model_path}/model.bin\")\n",
    "print(\"Model located at {}\".format(model_path))\n",
    "print(\"Quantized model located at {}\".format(quantized_model_path))\n",
    "\n",
    "core = Core()\n",
    "read_model = core.read_model(model=model, weights=weights)\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\")\n",
    "read_quantized_model = core.read_model(model=quantized_model, weights=quantized_weights)\n",
    "compiled_quantized_model = core.compile_model(model=quantized_model, device_name=\"CPU\")\n",
    "\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "quantized_input_layer = compiled_quantized_model.input(0)\n",
    "quantized_output_layer = compiled_quantized_model.output(0)\n",
    "image = cv2.imread(filename=\"inference_test/images/image1.jpg\")\n",
    "# our model was trained on RGB ordered images but OpenCV represents\n",
    "# images in BGR order, so swap the channels,\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resize image to network input image shape\n",
    "resized_image = cv2.resize(src=image, dsize=(224, 224))\n",
    "\n",
    "# Transpose image to network input shape\n",
    "input_image = np.expand_dims(np.transpose(resized_image, (1, 0, 2)), 0)\n",
    "\n",
    "result = compiled_model(inputs={input_layer: input_image})[output_layer]\n",
    "quantized_result = compiled_quantized_model(inputs={quantized_input_layer: input_image})[quantized_output_layer]\n",
    "result_index = np.argmax(result)\n",
    "quantized_result_index = np.argmax(quantized_result)\n",
    "\n",
    "labels = config.CLASSES\n",
    "\n",
    "print(f\"The openvino model depicts {labels[result_index]}\")\n",
    "\n",
    "# draw the prediction on the output image\n",
    "text = \"IR model -> {}: {:.2f}%\".format(labels[result_index], result[result_index] * 100)\n",
    "cv2.putText(resized_image, text, (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "            (0, 255, 0), 2)\n",
    "# show the resized_image image\n",
    "cv2.imshow(\"Output\", resized_image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "print(f\"The openvino quantized model depicts {labels[quantized_result_index]}\")\n",
    "\n",
    "text = \"IR model -> {}: {:.2f}%\".format(labels[quantized_result_index], quantized_result[quantized_result_index] * 100)\n",
    "cv2.putText(resized_image, text, (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "            (0, 255, 0), 2)\n",
    "# show the resized_image image\n",
    "cv2.imshow(\"Output\", resized_image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Calculating WaverX-Vision in IR format benchmarks\")\n",
    "num_images = 1000\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    compiled_model(inputs={input_layer: input_image})\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(\n",
    "    f\"IR model in Inference Engine/CPU: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")\n",
    "\n",
    "print(\"Calculating WaverX-Vision in Quantized IR format Benchmarks\")\n",
    "num_images = 1000\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    compiled_quantized_model(inputs={quantized_input_layer: input_image})\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(\n",
    "    f\"IR quantized model in Inference Engine/CPU: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "443f9398-da68-453f-8fa4-793565653015",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv inference_test/test_computer_vision_model/* inference_test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4a2990d-f99a-47d1-b5e9-2e52dfd86ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automate_deployment\t    Dockerfile\t    model_dataset\n",
      "build_dataset.py\t    inference_test  plot.png\n",
      "convert_model.py\t    __init__.py     quantize_model.py\n",
      "dataset\t\t\t    logs\t    README.md\n",
      "dataset_filtering.log\t    model\t    requirements.txt\n",
      "disaster_detector_model.py  model_config    waverX-Vision.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b5e0f-2f61-4c1a-bb7f-da58dc1da9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intel TensorFlow & Quantization",
   "language": "python",
   "name": "oneapi-aikit-dlpackage-with-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
